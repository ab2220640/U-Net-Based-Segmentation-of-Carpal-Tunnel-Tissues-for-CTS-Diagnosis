import os
import glob
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, Subset
from torchvision import transforms
from PIL import Image
import numpy as np
from sklearn.model_selection import KFold
from tqdm import tqdm  # å°å…¥é€²åº¦æ¢å¥—ä»¶

# ==========================================
# è¨­å®šè£ç½®
# ==========================================
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ==========================================
# 1. æå¤±å‡½æ•¸ (Dice Loss)
# ==========================================
class DiceLoss(nn.Module):
    def __init__(self):
        super(DiceLoss, self).__init__()
 
    def forward(self, input, target):
        N = target.size(0)
        smooth = 1
        input_flat = input.view(N, -1)
        target_flat = target.view(N, -1)
        intersection = input_flat * target_flat
        loss = 2 * (intersection.sum(1) + smooth) / (input_flat.sum(1) + target_flat.sum(1) + smooth)
        loss = 1 - loss.sum() / N
        return loss

class MulticlassDiceLoss(nn.Module):
    def __init__(self):
        super(MulticlassDiceLoss, self).__init__()
 
    def forward(self, input, target, weights=None):
        C = target.shape[1]
        dice = DiceLoss()
        totalLoss = 0
        for i in range(C):
            diceLoss = dice(input[:,i], target[:,i])
            if weights is not None:
                diceLoss *= weights[i]
            totalLoss += diceLoss
        return totalLoss / C

# ==========================================
# 2. U-Net æ¨¡å‹æ¶æ§‹
# ==========================================
class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.double_conv(x)

class UNet(nn.Module):
    def __init__(self, n_channels, n_classes):
        super(UNet, self).__init__()
        self.n_channels = n_channels
        self.n_classes = n_classes
        self.bilinear = True

        self.inc = DoubleConv(n_channels, 64)
        self.down1 = DoubleConv(64, 128)
        self.down2 = DoubleConv(128, 256)
        self.down3 = DoubleConv(256, 512)
        factor = 2 if self.bilinear else 1
        self.down4 = DoubleConv(512, 1024 // factor)
        self.pool = nn.MaxPool2d(2)
        self.up1 = DoubleConv(1024, 512 // factor)
        self.up2 = DoubleConv(512, 256 // factor)
        self.up3 = DoubleConv(256, 128 // factor)
        self.up4 = DoubleConv(128, 64)
        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)
        self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)

    def forward(self, x):
        x1 = self.inc(x)
        x2 = self.down1(self.pool(x1))
        x3 = self.down2(self.pool(x2))
        x4 = self.down3(self.pool(x3))
        x5 = self.down4(self.pool(x4))
        
        x = self.up_sample(x5)
        diffY = x4.size()[2] - x.size()[2]
        diffX = x4.size()[3] - x.size()[3]
        x = nn.functional.pad(x, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])
        x = torch.cat([x4, x], dim=1)
        x = self.up1(x)

        x = self.up_sample(x)
        diffY = x3.size()[2] - x.size()[2]
        diffX = x3.size()[3] - x.size()[3]
        x = nn.functional.pad(x, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])
        x = torch.cat([x3, x], dim=1)
        x = self.up2(x)

        x = self.up_sample(x)
        diffY = x2.size()[2] - x.size()[2]
        diffX = x2.size()[3] - x.size()[3]
        x = nn.functional.pad(x, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])
        x = torch.cat([x2, x], dim=1)
        x = self.up3(x)

        x = self.up_sample(x)
        diffY = x1.size()[2] - x.size()[2]
        diffX = x1.size()[3] - x.size()[3]
        x = nn.functional.pad(x, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])
        x = torch.cat([x1, x], dim=1)
        x = self.up4(x)
        
        logits = self.outc(x)
        return torch.sigmoid(logits)

# ==========================================
# 3. è³‡æ–™é›†å®šç¾©
# ==========================================
class CarpalTunnelDataset(Dataset):
    def __init__(self, root_dir):
        self.root_dir = root_dir
        self.data_list = []
        
        for i in range(10): # 0~9 è³‡æ–™å¤¾
            case_path = os.path.join(root_dir, str(i))
            # æœå°‹åœ–ç‰‡ï¼Œæ”¯æ´ jpg æˆ– png
            t1_files = sorted(glob.glob(os.path.join(case_path, 'T1', '*.*')))
            
            for t1_path in t1_files:
                filename = os.path.basename(t1_path)
                t2_path = os.path.join(case_path, 'T2', filename)
                mn_path = os.path.join(case_path, 'MN', filename)
                ft_path = os.path.join(case_path, 'FT', filename)
                ct_path = os.path.join(case_path, 'CT', filename)
                
                if os.path.exists(t2_path):
                    self.data_list.append({
                        't1': t1_path, 't2': t2_path,
                        'mn': mn_path, 'ft': ft_path, 'ct': ct_path
                    })

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx):
        item = self.data_list[idx]
        
        t1_img = Image.open(item['t1']).convert('L')
        t2_img = Image.open(item['t2']).convert('L')
        
        def read_mask(path):
            if os.path.exists(path):
                return Image.open(path).convert('L')
            else:
                return Image.new('L', t1_img.size, 0)
        
        mn_mask = read_mask(item['mn'])
        ft_mask = read_mask(item['ft'])
        ct_mask = read_mask(item['ct'])
        
        to_tensor = transforms.ToTensor()
        
        # çµ„åˆå½±åƒ (2 channels)
        image = torch.cat((to_tensor(t1_img), to_tensor(t2_img)), dim=0)
        # çµ„åˆ Mask (3 channels: MN, FT, CT)
        mask = torch.cat((to_tensor(mn_mask), to_tensor(ft_mask), to_tensor(ct_mask)), dim=0)
        mask = (mask > 0.5).float() # äºŒå€¼åŒ–

        return image, mask

# ==========================================
# 4. è¨“ç·´èˆ‡è¼”åŠ©åŠŸèƒ½
# ==========================================
def save_checkpoint(state, filename="my_checkpoint.pth.tar"):
    torch.save(state, filename)

def train_one_epoch(loader, model, optimizer, loss_fn, scaler, epoch_idx):
    # ä½¿ç”¨ tqdm åŒ…è£ loader ä»¥é¡¯ç¤ºé€²åº¦æ¢
    loop = tqdm(loader, desc=f"Epoch {epoch_idx}", leave=False)
    model.train()
    epoch_loss = 0
    
    for batch_idx, (data, targets) in enumerate(loop):
        data = data.to(device)
        targets = targets.to(device)

        # æ··åˆç²¾åº¦è¨“ç·´ (ç¯€çœ 4060 é¡¯å­˜)
        with torch.cuda.amp.autocast():
            predictions = model(data)
            loss = loss_fn(predictions, targets)

        optimizer.zero_grad()
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        epoch_loss += loss.item()
        
        # æ›´æ–°é€²åº¦æ¢å¾Œé¢çš„è³‡è¨Š
        loop.set_postfix(loss=loss.item())
        
    return epoch_loss / len(loader)

def check_accuracy(loader, model, device="cuda"):
    dice_loss_fn = MulticlassDiceLoss()
    model.eval()
    dice_score = 0
    num_batches = 0
    
    # é©—è­‰æ™‚ä¸éœ€è¦è¨ˆç®—æ¢¯åº¦
    with torch.no_grad():
        for x, y in loader:
            x = x.to(device)
            y = y.to(device)
            preds = model(x)
            preds = (preds > 0.5).float()
            
            loss = dice_loss_fn(preds, y)
            dice_score += (1 - loss.item())
            num_batches += 1
            
    model.train()
    if num_batches == 0: return 0
    return dice_score / num_batches

def main():
    print(f"âœ… ç³»çµ±åµæ¸¬åˆ°: {torch.cuda.get_device_name(0)}")
    print("ğŸš€ é–‹å§‹æº–å‚™è¨“ç·´æµç¨‹...")

    # --- è¼•é‡åŒ–åƒæ•¸è¨­å®š (é‡å° RTX 4060 + i7) ---
    LEARNING_RATE = 1e-4
    BATCH_SIZE = 4      # 4060 8GB å»ºè­°è¨­ç‚º 4ï¼Œè‹¥å‡ºç¾ OOM (è¨˜æ†¶é«”ä¸è¶³) è«‹æ”¹ç‚º 2
    NUM_EPOCHS = 50     # æ¯å€‹ Fold è·‘ 50 è¼ª
    NUM_WORKERS = 2     # ä½¿ç”¨ 2 å€‹ CPU æ ¸å¿ƒè®€åœ–ï¼Œé¿å…é›»è…¦å¡é “
    NUM_FOLDS = 5       # 5æŠ˜äº¤å‰é©—è­‰
    DATA_DIR = "./data" # è«‹ç¢ºä¿è³‡æ–™å¤¾çµæ§‹æ­£ç¢º
    
    # è¼‰å…¥è³‡æ–™é›†
    full_dataset = CarpalTunnelDataset(DATA_DIR)
    print(f"ğŸ“‚ ç¸½å…±è¼‰å…¥ {len(full_dataset)} çµ„å½±åƒè³‡æ–™")
    
    kfold = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)
    
    # è®€å–è¨“ç·´é€²åº¦ (ä¸­æ–·çºŒç·´åŠŸèƒ½)
    start_fold = 0
    fold_record_file = "fold_status.txt"
    if os.path.exists(fold_record_file):
        with open(fold_record_file, "r") as f:
            content = f.read().strip()
            if content:
                start_fold = int(content)
        print(f"ğŸ”„ åµæ¸¬åˆ°ä¸Šæ¬¡é€²åº¦ï¼Œå°‡å¾ Fold {start_fold + 1} ç¹¼çºŒè¨“ç·´...")

    # --- Fold è¿´åœˆ ---
    for fold, (train_ids, test_ids) in enumerate(kfold.split(full_dataset)):
        if fold < start_fold:
            continue
            
        print(f'\n========================================')
        print(f'ğŸ”¥ ç¾åœ¨é–‹å§‹ Fold {fold+1}/{NUM_FOLDS}')
        print(f'========================================')
        
        train_subsampler = Subset(full_dataset, train_ids)
        test_subsampler = Subset(full_dataset, test_ids)
        
        # Windows ç³»çµ±ä¸‹ï¼ŒDataLoader éœ€è¦åœ¨ if __name__ == '__main__': å€å¡Šå…§é‹è¡Œ
        train_loader = DataLoader(train_subsampler, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)
        test_loader = DataLoader(test_subsampler, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)
        
        model = UNet(n_channels=2, n_classes=3).to(device)
        loss_fn = MulticlassDiceLoss()
        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
        scaler = torch.cuda.amp.GradScaler()

        # è®€å–è©² Fold çš„æš«å­˜æª” (Checkpoint)
        checkpoint_file = f"checkpoint_fold_{fold}.pth.tar"
        start_epoch = 0
        best_val_dice = 0
        
        if os.path.exists(checkpoint_file):
            print(f"ğŸ“¥ è¼‰å…¥ Fold {fold+1} çš„å­˜æª”é»...")
            checkpoint = torch.load(checkpoint_file)
            model.load_state_dict(checkpoint['state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer'])
            start_epoch = checkpoint['epoch']
            best_val_dice = checkpoint.get('best_dice', 0)

        # --- Epoch è¿´åœˆ ---
        for epoch in range(start_epoch, NUM_EPOCHS):
            avg_loss = train_one_epoch(train_loader, model, optimizer, loss_fn, scaler, epoch+1)
            val_dice = check_accuracy(test_loader, model, device=device)
            
            # ä½¿ç”¨ tqdm.write é¿å…æ‰“äº‚é€²åº¦æ¢
            tqdm.write(f"ğŸ“Š Fold {fold+1} | Epoch {epoch+1}/{NUM_EPOCHS} | Loss: {avg_loss:.4f} | Val Dice: {val_dice:.4f}")

            # å„²å­˜æš«å­˜æª” (æ¯æ¬¡éƒ½å­˜ï¼Œé˜²æ–·é›»)
            checkpoint = {
                'state_dict': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'epoch': epoch + 1,
                'best_dice': best_val_dice
            }
            save_checkpoint(checkpoint, filename=checkpoint_file)

            # å„²å­˜æœ€ä½³æ¨¡å‹ (åªå­˜è¡¨ç¾æœ€å¥½çš„)
            if val_dice > best_val_dice:
                best_val_dice = val_dice
                torch.save(model.state_dict(), f"best_model_fold_{fold}.pth")
                tqdm.write(f"ğŸ’¾ ç™¼ç¾æ›´ä½³æ¨¡å‹ï¼å·²å„²å­˜ (Dice: {best_val_dice:.4f})")
        
        # è©² Fold å®Œæˆï¼Œæ›´æ–°é€²åº¦ç´€éŒ„
        with open(fold_record_file, "w") as f:
            f.write(str(fold + 1))
            
        print(f"âœ… Fold {fold+1} è¨“ç·´çµæŸï¼æœ€ä½³ Dice Score: {best_val_dice:.4f}")

    print("\nğŸ‰ å…¨æ•¸è¨“ç·´å®Œæˆï¼æª”æ¡ˆå·²ç”Ÿæˆã€‚")

if __name__ == "__main__":
    # Windows å¿…é ˆåŠ é€™è¡Œä¿è­·ï¼Œå¦å‰‡å¤šåŸ·è¡Œç·’ (num_workers) æœƒå ±éŒ¯
    main()
